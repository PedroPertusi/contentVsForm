{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04800311",
   "metadata": {},
   "source": [
    "### 0. Imports and auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714410ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(\"TAALED_1_4_1_Py3\") \n",
    "sys.path.append(\"TAACO\")            \n",
    "\n",
    "# --- Core libs ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import glob, tempfile\n",
    "\n",
    "# --- HuggingFace (BERT) ---\n",
    "from transformers import BertTokenizerFast, BertModel\n",
    "import torch\n",
    "\n",
    "# --- Progress bar ---\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# --- spaCy compatibility (older TAACO/TAALED expect this) ---\n",
    "import spacy\n",
    "if not hasattr(spacy.util, \"set_data_path\"):\n",
    "    spacy.util.set_data_path = lambda *a, **k: None\n",
    "\n",
    "# --- TAASSC (your local module) ---\n",
    "import TAASSC_215_dev as tdev\n",
    "from TAASSC_215_dev import LGR_Analysis, index_list\n",
    "\n",
    "# --- TAALED setup (needs GUI stubs) ---\n",
    "import TAALED_1_4_1 as TAALED\n",
    "class _Root:\n",
    "    def update_idletasks(self): pass\n",
    "TAALED.root = _Root()\n",
    "TAALED.system = \"L\" \n",
    "\n",
    "# --- TAACO setup (import + GUI stubs + resource path) ---\n",
    "import TAACOnoGUI\n",
    "from TAACOnoGUI import runTAACO\n",
    "TAACOnoGUI.root = _Root()\n",
    "TAACOnoGUI.system = \"L\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8edd8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_df(path: str) -> pd.DataFrame:\n",
    "    \"\"\"Read DataFrame from CSV or Parquet based on file extension.\"\"\"\n",
    "    if path.endswith(\".parquet\"):\n",
    "        return pd.read_parquet(path)\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "def save_csv(df, path):\n",
    "    \"\"\"Save DataFrame as CSV and print confirmation.\"\"\"\n",
    "    Path(path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(path, index=False)\n",
    "    print(f\"Saved: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d6b06f",
   "metadata": {},
   "source": [
    "---- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62c009f",
   "metadata": {},
   "source": [
    "### 0. Define witch data we are preprocessing (input data)\n",
    "- Original data source (includes cleaning + preprocessing steps)\n",
    "- Rewritten text/data (only preprocessing steps)\n",
    "\n",
    "##### Setting up the reading and saving directory\n",
    "- Input data choice: original/rewritten\n",
    "- Input directory: where the data is stored (original/rewritten)\n",
    "- Output directory: where the preprocessed data will be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab7bebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== RUN SETTINGS ====\n",
    "MODE = \"original\" # \"original\" or \"rewrites\"\n",
    "\n",
    "# Original data\n",
    "ORIGINAL_PATH = \"../data/persuade/persuade_2.0_human_scores_demo_id_github.csv\"\n",
    "SAVE_LOW_HIGH = True\n",
    "COLUMNS_KEEP = ['full_text', 'holistic_essay_score', 'race_ethnicity', 'gender', 'grade_level', 'economically_disadvantaged', 'prompt_name']\n",
    "\n",
    "# Rewrites data\n",
    "REWRITES_FOLDER = \"../data/raw/rewrites\"\n",
    "REWRITES_PATTERN = \"*.csv\"   \n",
    "\n",
    "# Outputs\n",
    "OUT_DIR = \"../data/processed/run_01\"\n",
    "TEXT_COL = \"text\"\n",
    "\n",
    "RUN_TAALED = True\n",
    "RUN_TAACO = True\n",
    "RUN_TAASSC = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2baac284",
   "metadata": {},
   "source": [
    "---- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c290424",
   "metadata": {},
   "source": [
    "### 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c2f116",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE == \"original\":\n",
    "    print(\"Loading original dataset\")\n",
    "    df = read_df(ORIGINAL_PATH)\n",
    "elif MODE == \"rewrites\":\n",
    "    print(\"Loading rewrite datasets\")\n",
    "    paths = sorted(glob.glob(os.path.join(REWRITES_FOLDER, REWRITES_PATTERN)))\n",
    "    dfs = [read_df(p) for p in paths]\n",
    "    print(f\"Loaded {len(dfs)} rewrite files\")\n",
    "    \n",
    "else:\n",
    "    raise ValueError(\"MODE must be 'original' or 'rewrites'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319641e8",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22439254",
   "metadata": {},
   "source": [
    "### 2. Data cleaning (Only for original data source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961e2e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE == \"original\":\n",
    "    df = df[COLUMNS_KEEP]\n",
    "    df = df.dropna()\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df = df.rename(columns={'full_text': 'text'})\n",
    "    df['economically_disadvantaged'] = df['economically_disadvantaged'].map({'Economically disadvantaged': 1, 'Not economically disadvantaged': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3179ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9279650",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccebcf98",
   "metadata": {},
   "source": [
    "### 3. Data Preprocessing Steps (NLP tools)\n",
    "- TAALED\n",
    "- TAACCO\n",
    "- TAASSCC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc128aa",
   "metadata": {},
   "source": [
    "### TAALED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3765a840",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not hasattr(spacy.util, \"set_data_path\"):\n",
    "    spacy.util.set_data_path = lambda *a, **k: None\n",
    "class _Root:\n",
    "    def update_idletasks(self):  \n",
    "        pass\n",
    "\n",
    "TAALED.root = _Root()  # satisfy TAALED's references to a Tk root\n",
    "TAALED.system = \"M\"    # pretend we're on Linux/Mac ('L' or 'M'); avoids GUI branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c2c4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== TAALED runner (always 'taaled_' prefix) ====\n",
    "TAALED_VAR_DICT = {\n",
    "    \"aw\": 1, \"cw\": 1, \"fw\": 1,\n",
    "    \"simple_ttr\": 1, \"root_ttr\": 1, \"log_ttr\": 1, \"maas_ttr\": 1,\n",
    "    \"mattr\": 1, \"msttr\": 1, \"hdd\": 1,\n",
    "    \"mltd\": 1, \"mltd_ma\": 1, \"mtld_wrap\": 1,\n",
    "    \"indout\": 0,\n",
    "}\n",
    "\n",
    "def _detect_filename_col(res):\n",
    "    # Find filename column in TAALED's CSV\n",
    "    cands = {\"filename\",\"file\",\"file_name\",\"textname\",\"doc\",\"document\",\"name\"}\n",
    "    for c in res.columns:\n",
    "        if c.lower() in cands:\n",
    "            return c\n",
    "    return res.columns[0]\n",
    "\n",
    "def _run_taaled_once(df, *, text_col, var_dict):\n",
    "    # Run TAALED on one DataFrame and merge taaled_* metrics back\n",
    "    if text_col not in df.columns:\n",
    "        raise ValueError(f\"TEXT_COL='{text_col}' not found in columns: {list(df.columns)}\")\n",
    "    df = df.copy()\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "        for i, txt in df[text_col].items():\n",
    "            with open(os.path.join(tmp_dir, f\"{i}.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(txt if isinstance(txt, str) else \"\")\n",
    "\n",
    "        out_csv = os.path.join(tmp_dir, \"taaled_out.csv\")\n",
    "        TAALED.main(tmp_dir, out_csv, var_dict)\n",
    "        res = pd.read_csv(out_csv)\n",
    "\n",
    "    fn_col = _detect_filename_col(res)\n",
    "    res[\"__idx__\"] = res[fn_col].astype(str).str.replace(\".txt\", \"\", regex=False)\n",
    "    df[\"__idx__\"] = df.index.astype(str)\n",
    "\n",
    "    metric_cols = [c for c in res.columns if c not in (fn_col, \"__idx__\")]\n",
    "    res = res.rename(columns={c: f\"taaled_{c}\" for c in metric_cols})\n",
    "\n",
    "    merged = df.merge(res.drop(columns=[fn_col]), on=\"__idx__\", how=\"left\")\n",
    "    return merged.drop(columns=\"__idx__\")\n",
    "\n",
    "def run_taaled(df_or_list, *, mode, text_col, var_dict):\n",
    "    if mode == \"original\":\n",
    "        return _run_taaled_once(df_or_list, text_col=text_col, var_dict=var_dict)\n",
    "    elif mode == \"rewrites\":\n",
    "        return [_run_taaled_once(d, text_col=text_col, var_dict=var_dict) for d in df_or_list]\n",
    "    else:\n",
    "        raise ValueError(\"mode must be 'original' or 'rewrites'\")\n",
    "\n",
    "# ---- EXECUTE ----\n",
    "if RUN_TAALED:\n",
    "    if MODE == \"original\":\n",
    "        df = run_taaled(df, mode=\"original\", text_col=TEXT_COL, var_dict=TAALED_VAR_DICT)\n",
    "    elif MODE == \"rewrites\":\n",
    "        # Ensure each rewrite df has TEXT_COL; rename here if your files use another name\n",
    "        for i, d in enumerate(dfs):\n",
    "            if TEXT_COL not in d.columns and \"full_text\" in d.columns:\n",
    "                dfs[i] = d.rename(columns={\"full_text\": TEXT_COL})\n",
    "        dfs = run_taaled(dfs, mode=\"rewrites\", text_col=TEXT_COL, var_dict=TAALED_VAR_DICT)\n",
    "    print(\"TAALED done.\")\n",
    "else:\n",
    "    print(\"RUN_TAALED is False — skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbffeb26",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7426b9",
   "metadata": {},
   "source": [
    "### TAACO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f40a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "opts = {\n",
    "    \"sourceKeyOverlap\": False, \"sourceLSA\": False, \"sourceLDA\": False, \"sourceWord2vec\": False,\n",
    "    \"wordsAll\": True, \"wordsContent\": True, \"wordsFunction\": True,\n",
    "    \"wordsNoun\": True, \"wordsPronoun\": True, \"wordsArgument\": True,\n",
    "    \"wordsVerb\": True, \"wordsAdjective\": True, \"wordsAdverb\": True,\n",
    "    \"overlapSentence\": True, \"overlapParagraph\": True,\n",
    "    \"overlapAdjacent\": True, \"overlapAdjacent2\": True,\n",
    "    \"otherTTR\": True, \"otherConnectives\": True, \"otherGivenness\": True,\n",
    "    \"overlapLSA\": True, \"overlapLDA\": True, \"overlapWord2vec\": True,\n",
    "    \"overlapSynonym\": True, \"overlapNgrams\": True,\n",
    "    \"outputTagged\": False, \"outputDiagnostic\": False,\n",
    "}\n",
    "\n",
    "# ==== TAACO runner (simplified, always 'taaco_' prefix) ====\n",
    "\n",
    "def _detect_taaco_filename_col(res):\n",
    "    \"\"\"Find which column in TAACO's CSV contains the filenames.\"\"\"\n",
    "    candidates = {\"filename\",\"file\",\"file_name\",\"textname\",\"doc\",\"document\",\"name\"}\n",
    "    for c in res.columns:\n",
    "        if c.lower() in candidates:\n",
    "            return c\n",
    "    for c in res.columns:  # fallback: looks like '*.txt'\n",
    "        try:\n",
    "            if res[c].astype(str).str.endswith(\".txt\").any():\n",
    "                return c\n",
    "        except Exception:\n",
    "            pass\n",
    "    return res.columns[0]\n",
    "\n",
    "def _run_taaco_once(df, *, text_col, opts):\n",
    "    \"\"\"Run TAACO on a single DataFrame and merge taaco_ metrics back.\"\"\"\n",
    "    if text_col not in df.columns:\n",
    "        raise ValueError(f\"TEXT_COL='{text_col}' not found\")\n",
    "    df = df.copy()\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "        for i, txt in df[text_col].items():\n",
    "            with open(os.path.join(tmp_dir, f\"{i}.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(txt if isinstance(txt, str) else \"\")\n",
    "\n",
    "        out_csv = os.path.join(tmp_dir, \"taaco_out.csv\")\n",
    "        runTAACO(tmp_dir, out_csv, opts)\n",
    "        res = pd.read_csv(out_csv)\n",
    "\n",
    "    fn_col = _detect_taaco_filename_col(res)\n",
    "    res[\"__idx__\"] = res[fn_col].astype(str).str.replace(\".txt\", \"\", regex=False)\n",
    "    df[\"__idx__\"] = df.index.astype(str)\n",
    "\n",
    "    metric_cols = [c for c in res.columns if c not in (fn_col, \"__idx__\")]\n",
    "    res = res.rename(columns={c: f\"taaco_{c}\" for c in metric_cols})\n",
    "\n",
    "    merged = df.merge(res.drop(columns=[fn_col]), on=\"__idx__\", how=\"left\")\n",
    "    return merged.drop(columns=\"__idx__\")\n",
    "\n",
    "def run_taaco(df_or_list, *, mode, text_col, opts):\n",
    "    \"\"\"Handles both modes: DataFrame (original) or list[DataFrame] (rewrites).\"\"\"\n",
    "    if mode == \"original\":\n",
    "        return _run_taaco_once(df_or_list, text_col=text_col, opts=opts)\n",
    "    elif mode == \"rewrites\":\n",
    "        return [_run_taaco_once(d, text_col=text_col, opts=opts) for d in df_or_list]\n",
    "    else:\n",
    "        raise ValueError(\"mode must be 'original' or 'rewrites'\")\n",
    "\n",
    "# ---- EXECUTE ----\n",
    "if RUN_TAACO:\n",
    "    # Save current working directory and switch into TAACO folder\n",
    "    orig_cwd = os.getcwd()\n",
    "    taaco_dir = \"TAACO\"  \n",
    "    os.chdir(taaco_dir)\n",
    "\n",
    "    try:\n",
    "        if MODE == \"original\":\n",
    "            df = run_taaco(df, mode=\"original\", text_col=TEXT_COL, opts=opts)\n",
    "\n",
    "        elif MODE == \"rewrites\":\n",
    "            for i, d in enumerate(dfs):\n",
    "                if TEXT_COL not in d.columns and \"full_text\" in d.columns:\n",
    "                    dfs[i] = d.rename(columns={\"full_text\": TEXT_COL})\n",
    "            dfs = run_taaco(dfs, mode=\"rewrites\", text_col=TEXT_COL, opts=opts)\n",
    "\n",
    "        print(\"TAACO done.\")\n",
    "\n",
    "    finally:\n",
    "        # Always restore the original directory even if TAACO crashes\n",
    "        os.chdir(orig_cwd)\n",
    "        print(f\"→ reverted working dir to: {os.getcwd()}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a78762",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7c754d",
   "metadata": {},
   "source": [
    "### TAASSC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e571e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== TAASSC runner (simplified, always 'taassc_' prefix) ====\n",
    "\n",
    "def _run_taassc_once(df, *, text_col, index_list):\n",
    "    \"\"\"Run TAASSC (LGR_Analysis) on one DataFrame and merge 'taassc_' metrics back.\"\"\"\n",
    "    if text_col not in df.columns:\n",
    "        raise ValueError(f\"TEXT_COL='{text_col}' not found in columns: {list(df.columns)[:12]}...\")\n",
    "    df = df.copy()\n",
    "\n",
    "    records = []\n",
    "    for txt in df[text_col].fillna(\"\"):\n",
    "        try:\n",
    "            res = LGR_Analysis(txt)  # dict of TAASSC metrics\n",
    "            row = {m: res.get(m, float('nan')) for m in index_list}\n",
    "        except Exception:\n",
    "            row = {m: float('nan') for m in index_list}\n",
    "        records.append(row)\n",
    "\n",
    "    metrics_df = pd.DataFrame.from_records(records, index=df.index)\n",
    "    metrics_df.columns = [f\"taassc_{m}\" for m in index_list]\n",
    "\n",
    "    return pd.concat([df, metrics_df], axis=1)\n",
    "\n",
    "def run_taassc(df_or_list, *, mode, text_col, index_list):\n",
    "    \"\"\"Handles both modes: DataFrame (original) or list[DataFrame] (rewrites).\"\"\"\n",
    "    if mode == \"original\":\n",
    "        return _run_taassc_once(df_or_list, text_col=text_col, index_list=index_list)\n",
    "    elif mode == \"rewrites\":\n",
    "        return [_run_taassc_once(d, text_col=text_col, index_list=index_list) for d in df_or_list]\n",
    "    else:\n",
    "        raise ValueError(\"mode must be 'original' or 'rewrites'\")\n",
    "\n",
    "# ---- EXECUTE ----\n",
    "if RUN_TAASSC:\n",
    "    if MODE == \"original\":\n",
    "        df = run_taassc(df, mode=\"original\", text_col=TEXT_COL, index_list=index_list)\n",
    "    elif MODE == \"rewrites\":\n",
    "        # Ensure each rewrite df has TEXT_COL; rename here if needed\n",
    "        for i, d in enumerate(dfs):\n",
    "            if TEXT_COL not in d.columns and \"full_text\" in d.columns:\n",
    "                dfs[i] = d.rename(columns={\"full_text\": TEXT_COL})\n",
    "        dfs = run_taassc(dfs, mode=\"rewrites\", text_col=TEXT_COL, index_list=index_list)\n",
    "    print(\"TAASSC done.\")\n",
    "else:\n",
    "    print(\"RUN_TAASSC is False — skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a60a97",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf56d0f",
   "metadata": {},
   "source": [
    "### 4. Save Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba428a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== SAVE CLEANED DATASETS (CSV only) ====\n",
    "\n",
    "if MODE == \"original\":\n",
    "    print(\"Saving original dataset and SES splits...\")\n",
    "    base = Path(OUT_DIR)\n",
    "    base.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Full version\n",
    "    save_csv(df, base / \"original_full.csv\")\n",
    "\n",
    "    # Low / High SES splits\n",
    "    if SAVE_LOW_HIGH:\n",
    "        low = df[df[\"economically_disadvantaged\"] == 1].reset_index(drop=True)\n",
    "        high = df[df[\"economically_disadvantaged\"] == 0].reset_index(drop=True)\n",
    "        save_csv(low, base / \"original_low_SES.csv\")\n",
    "        save_csv(high, base / \"original_high_SES.csv\")\n",
    "        \n",
    "elif MODE == \"rewrites\":\n",
    "    print(\"Saving all rewrite datasets...\")\n",
    "    base = Path(OUT_DIR)\n",
    "    base.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for i, d in enumerate(dfs):\n",
    "        name = f\"rewrite_{i+1}.csv\"\n",
    "        save_csv(d, base / name)\n",
    "\n",
    "    print(f\"Saved {len(dfs)} rewritten datasets to {base}\")\n",
    "\n",
    "print(\"All saves complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6906011",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1409dd",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e487235c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps') if (torch.backends.mps.is_available()) else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased').eval().to(device)\n",
    "\n",
    "def get_embeddings(text: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Run one text through BERT, return the [CLS] embedding as a numpy vector.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text,\n",
    "                       return_tensors='pt',\n",
    "                       padding=True,\n",
    "                       truncation=True,\n",
    "                       max_length=512)\n",
    "    # move to device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # [batch=1, seq, dim] → pick CLS token embedding\n",
    "    cls_emb = outputs.last_hidden_state[0, 0, :]\n",
    "    return cls_emb.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9e6e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== EMBEDDINGS SAVE (BERT [CLS]) ====\n",
    "def _embed_df(df, text_col):\n",
    "    texts = df[text_col].fillna(\"\").tolist()\n",
    "    it = texts\n",
    "    if \"tqdm\" in globals():  # optional progress bar if you already imported tqdm\n",
    "        it = tqdm(texts, desc=\"Embedding\", total=len(texts))\n",
    "    embs = [get_embeddings(t) for t in it]\n",
    "    return np.vstack(embs) if len(embs) else np.zeros((0, model.config.hidden_size), dtype=float)\n",
    "\n",
    "emb_dir = Path(OUT_DIR) / \"embeddings\"\n",
    "emb_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if MODE == \"original\":\n",
    "    X_full = _embed_df(df, TEXT_COL)\n",
    "    np.save(emb_dir / \"embeddings_original_full.npy\", X_full)\n",
    "\n",
    "    if SAVE_LOW_HIGH:\n",
    "        low = df[df[\"economically_disadvantaged\"] == 1].reset_index(drop=True)\n",
    "        high = df[df[\"economically_disadvantaged\"] == 0].reset_index(drop=True)\n",
    "\n",
    "        X_low = _embed_df(low, TEXT_COL) if len(low) else np.zeros((0, X_full.shape[1]), dtype=float)\n",
    "        X_high = _embed_df(high, TEXT_COL) if len(high) else np.zeros((0, X_full.shape[1]), dtype=float)\n",
    "\n",
    "        np.save(emb_dir / \"embeddings_original_low.npy\", X_low)\n",
    "        np.save(emb_dir / \"embeddings_original_high.npy\", X_high)\n",
    "\n",
    "        print(f\"Saved embeddings to {emb_dir}\")\n",
    "\n",
    "elif MODE == \"rewrites\":\n",
    "    \n",
    "    items = [(f\"rewrite_{i+1}\", d) for i, d in enumerate(dfs)]\n",
    "\n",
    "    for name, d in items:\n",
    "        # Ensure expected text column\n",
    "        if TEXT_COL not in d.columns and \"full_text\" in d.columns:\n",
    "            d = d.rename(columns={\"full_text\": TEXT_COL})\n",
    "        if TEXT_COL not in d.columns:\n",
    "            raise KeyError(f\"{name}: expected '{TEXT_COL}' column. Got: {list(d.columns)[:12]}...\")\n",
    "\n",
    "        X = _embed_df(d, TEXT_COL)\n",
    "        np.save(emb_dir / f\"embeddings_{name}.npy\", X)\n",
    "        print(f\"{name}: saved {X.shape} to {emb_dir / f'embeddings_{name}.npy'}\")\n",
    "\n",
    "    print(f\"Saved {len(items)} rewrite embeddings to {emb_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
