{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347006bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from string import Template\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI, AzureOpenAI\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "LIMIT_ROWS = None\n",
    "TEMPERATURE = 1\n",
    "N = 6\n",
    "MAX_TOKENS = 2048\n",
    "\n",
    "PERSUADE_PATH = \"../data/persuade/persuade_full_cleaned.csv\"\n",
    "\n",
    "PROMPT = \"no_descriptions\"\n",
    "\n",
    "SAVE_DIR = \"../data/rewrites/no_desc/\"\n",
    "SAVE_NAME = \"no_desc_full\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8193997",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "if not api_key:\n",
    "    raise RuntimeError(\"Missing API_KEY in environment\")\n",
    "\n",
    "endpoint = \"https://extractionhub.cognitiveservices.azure.com/\"\n",
    "model_name = \"gpt-4o\"\n",
    "deployment = \"gpt-4o\"   \n",
    "api_version = \"2024-10-21\"\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_version=api_version,\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99076cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(PERSUADE_PATH)\n",
    "\n",
    "if LIMIT_ROWS:\n",
    "    df = df.head(LIMIT_ROWS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c637e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prompt_template(filename: str) -> Template:\n",
    "    path = Path(f\"../prompts/{filename}.txt\")\n",
    "    text = path.read_text(encoding=\"utf-8\")\n",
    "    return Template(text)\n",
    "\n",
    "def build_rewrite_prompt(\n",
    "    prompt_features: dict,\n",
    "    filename: str) -> str:\n",
    "    tpl = load_prompt_template(f\"{filename}\")\n",
    "\n",
    "    tpl = tpl.safe_substitute(prompt_features)\n",
    "\n",
    "    return tpl\n",
    "\n",
    "def call_llm_optimizer(prompt: str, content: str):\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=deployment,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": content},\n",
    "            {\"role\": \"user\",   \"content\": prompt},\n",
    "        ],\n",
    "        temperature=TEMPERATURE,\n",
    "        max_tokens=MAX_TOKENS,\n",
    "        n=N,\n",
    "    )\n",
    "\n",
    "    outputs = []\n",
    "    for choice in response.choices:\n",
    "        text = choice.message.content.strip()\n",
    "        outputs.append(text)\n",
    "\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91c2149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_evauation_prompt(original_text: str, rewritten_texts: str, file: str) -> str:\n",
    "    path = Path(f\"../prompts/evaluation/{file}.txt\")\n",
    "    text = path.read_text(encoding=\"utf-8\")\n",
    "    template = Template(text)\n",
    "\n",
    "    rewritten_texts = \"\\n\".join([f\"- Text {i}: \\n\\n\" + text for i, text in enumerate(rewritten_texts)])\n",
    "    return template.substitute(original_text=original_text, rewritten_texts=rewritten_texts)\n",
    "\n",
    "def call_llm_evaluator(prompt: str, temperature: float = 0.0, max_tokens: int = 1):\n",
    "    response = client.chat.completions.create(\n",
    "        model=deployment,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"You are a text evaluation specialist with expertise in \"\n",
    "                    \"socioeconomic style transfer.\"\n",
    "                )\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=TEMPERATURE,\n",
    "        max_tokens=MAX_TOKENS,\n",
    "        n=1,\n",
    "    )\n",
    "\n",
    "    verdicts = ast.literal_eval(response.choices[0].message.content)\n",
    "    return verdicts \n",
    "\n",
    "def evaluate_rewrites(original_text: str, rewritten_texts: list[str], file: str) -> pd.DataFrame:\n",
    "    return call_llm_evaluator(build_evauation_prompt(original_text, rewritten_texts, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f68c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    prompt_features = {\n",
    "        \"text\": row['text']\n",
    "    }\n",
    "    full_prompt = build_rewrite_prompt(prompt_features, PROMPT)\n",
    "\n",
    "    content = \"You are an expert text rewriter. Your sole job is to rewrite the given text according to the style instructions in the user message.\"\n",
    "    try:\n",
    "        outputs = call_llm_optimizer(full_prompt, content)\n",
    "\n",
    "        try:\n",
    "            content_preserved = evaluate_rewrites(original_text=row['text'], rewritten_texts=outputs, file=\"evaluation_function\")\n",
    "\n",
    "            for i, cp in enumerate((content_preserved)):\n",
    "                if content_preserved[i] == 'YES':\n",
    "                    df.at[index, f'content_preserved_{i}'] = True\n",
    "                else:\n",
    "                    df.at[index, f'content_preserved_{i}'] = False\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            df.at[index, 'content_preserved_0'] = None\n",
    "\n",
    "        for i, texts in enumerate(outputs):\n",
    "            df.at[index, f'rewritten_text_{i}'] = outputs[i]\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in evaluation for index {index}: {e}\")\n",
    "        df.at[index, 'rewritten_text'] = 'error occurred'\n",
    "        df.at[index, 'content_preserved'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7becf417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df_combined to csv\n",
    "df.to_csv(f'{SAVE_DIR}{SAVE_NAME}', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149b4ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rewritten_text'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34833586",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df[df['rewritten_text'] != 'error ocurred']\n",
    "df_cleaned.to_csv(f'{SAVE_DIR}{SAVE_NAME}_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75887d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_error = df[df['rewritten_text'] == 'error occurred']\n",
    "df_error.to_csv(f'{SAVE_DIR}{SAVE_NAME}_errors.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e14ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned[[f'content_preserved_{i}' for i in range(6)]].stack().value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
