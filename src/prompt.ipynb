{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "347006bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from string import Template\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import AzureOpenAI\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import ast\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "LIMIT_ROWS = None\n",
    "TEMPERATURE = 1\n",
    "N = 6\n",
    "MAX_TOKENS = 2048\n",
    "\n",
    "PERSUADE_PATH = \"../data/persuade/persuade_full_cleaned.csv\"\n",
    "\n",
    "PROMPT_DIR = \"../prompts/sat/\"\n",
    "USER_PROMPTS_PREFIX = \"sat_score_{}.txt\"\n",
    "\n",
    "EVALUATION_PROMPT_PATH = \"../prompts/evaluation/evaluation_function.txt\"\n",
    "\n",
    "SAVE_DIR = \"../data/rewrites/sat_2/\"\n",
    "SAVE_NAME = \"rew_sat_{}.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "307f91c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT_PATH = PROMPT_DIR + \"system_prompt.txt\"\n",
    "\n",
    "# Load system prompt once\n",
    "with open(SYSTEM_PROMPT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    SYSTEM_PROMPT = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8193997",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "if not api_key:\n",
    "    raise RuntimeError(\"Missing API_KEY in environment\")\n",
    "\n",
    "endpoint = \"https://extractionhub.cognitiveservices.azure.com/\"\n",
    "model_name = \"gpt-4o\"\n",
    "deployment = \"gpt-4o\"   \n",
    "api_version = \"2024-10-21\"\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_version=api_version,\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99076cbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>holistic_essay_score</th>\n",
       "      <th>race_ethnicity</th>\n",
       "      <th>gender</th>\n",
       "      <th>grade_level</th>\n",
       "      <th>economically_disadvantaged</th>\n",
       "      <th>prompt_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Some schools require students to complete summ...</td>\n",
       "      <td>3</td>\n",
       "      <td>White</td>\n",
       "      <td>M</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Summer projects</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Letting teachers design the project is the mos...</td>\n",
       "      <td>4</td>\n",
       "      <td>Black/African American</td>\n",
       "      <td>M</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Summer projects</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Some schools implement a summer project to con...</td>\n",
       "      <td>4</td>\n",
       "      <td>White</td>\n",
       "      <td>M</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Summer projects</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Would you want to waste your summer on useless...</td>\n",
       "      <td>4</td>\n",
       "      <td>Asian/Pacific Islander</td>\n",
       "      <td>F</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Summer projects</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>During summer break, it's a time in which the ...</td>\n",
       "      <td>3</td>\n",
       "      <td>Hispanic/Latino</td>\n",
       "      <td>M</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Summer projects</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  holistic_essay_score  \\\n",
       "0  Some schools require students to complete summ...                     3   \n",
       "1  Letting teachers design the project is the mos...                     4   \n",
       "2  Some schools implement a summer project to con...                     4   \n",
       "3  Would you want to waste your summer on useless...                     4   \n",
       "4  During summer break, it's a time in which the ...                     3   \n",
       "\n",
       "           race_ethnicity gender  grade_level  economically_disadvantaged  \\\n",
       "0                   White      M         11.0                           0   \n",
       "1  Black/African American      M         11.0                           0   \n",
       "2                   White      M         11.0                           0   \n",
       "3  Asian/Pacific Islander      F         11.0                           0   \n",
       "4         Hispanic/Latino      M         11.0                           1   \n",
       "\n",
       "       prompt_name  \n",
       "0  Summer projects  \n",
       "1  Summer projects  \n",
       "2  Summer projects  \n",
       "3  Summer projects  \n",
       "4  Summer projects  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(PERSUADE_PATH)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f18e801",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lm/psfj50g95wgczw9z2l6zf32m0000gn/T/ipykernel_106/3855532549.py:4: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: x.sample(min(k, len(x)), replace=False))\n",
      "/var/folders/lm/psfj50g95wgczw9z2l6zf32m0000gn/T/ipykernel_106/3855532549.py:4: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: x.sample(min(k, len(x)), replace=False))\n"
     ]
    }
   ],
   "source": [
    "def stratified_fixed_k(df, label_col, k):\n",
    "    return (\n",
    "        df.groupby(label_col, group_keys=False)\n",
    "          .apply(lambda x: x.sample(min(k, len(x)), replace=False))\n",
    "    )\n",
    "\n",
    "K = 50\n",
    "\n",
    "df_high_ses = df[df[\"economically_disadvantaged\"] == 0]\n",
    "df_low_ses  = df[df[\"economically_disadvantaged\"] == 1]\n",
    "\n",
    "sampled_high_ses = stratified_fixed_k(df_high_ses, \"holistic_essay_score\", K)\n",
    "sampled_low_ses  = stratified_fixed_k(df_low_ses, \"holistic_essay_score\", K)\n",
    "\n",
    "df = pd.concat([sampled_high_ses, sampled_low_ses], ignore_index=True)\n",
    "df[\"rewritten_text\"] = None\n",
    "df[\"content_preserved\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4edb198",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_by_score = {\n",
    "    i: df.copy().reset_index(drop=True)\n",
    "    for i in range(1, 7)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68c637e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prompt_template(file: str) -> Template:\n",
    "    return Template(Path(file).read_text(encoding=\"utf-8\"))\n",
    "\n",
    "def build_prompt(prompt_features: dict, file: str) -> str:\n",
    "    tpl = load_prompt_template(file)\n",
    "    tpl = tpl.safe_substitute(prompt_features)\n",
    "    return tpl\n",
    "\n",
    "def call_llm_optimizer(system_prompt: str, user_prompt: str):\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=deployment,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        temperature=TEMPERATURE,\n",
    "        max_tokens=MAX_TOKENS,\n",
    "        n=N,\n",
    "    )\n",
    "\n",
    "    outputs = [choice.message.content.strip() for choice in response.choices]\n",
    "\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f91c2149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm_evaluator(user_prompt: str):\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=deployment,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a text evaluation specialist with expertise in socioeconomic style transfer.\"},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        temperature=TEMPERATURE,\n",
    "        max_tokens=MAX_TOKENS,\n",
    "        n=1,\n",
    "    )\n",
    "\n",
    "    verdicts = ast.literal_eval(response.choices[0].message.content)\n",
    "    return verdicts \n",
    "\n",
    "def evaluate_rewrites(original_text: str, rewritten_texts: list[str], file: str) -> pd.DataFrame:\n",
    "    prompt_features = {\"original_text\" : original_text}\n",
    "    \n",
    "    rew_txt = \"Texts: \\n\"\n",
    "    for i,text in enumerate(rewritten_texts):\n",
    "        rew_txt += f\"- Text {i}: \\n {text} \\n\"\n",
    "    prompt_features[\"rewritten_texts\"] = rew_txt\n",
    "\n",
    "    return call_llm_evaluator(build_prompt(prompt_features=prompt_features, file=file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b46b29a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_one_style(user_prompt: str, system_prompt: str):\n",
    "    \"\"\"\n",
    "    Calls the optimizer for a single style prompt and returns a single rewritten text.\n",
    "    \"\"\"\n",
    "    outputs = call_llm_optimizer(system_prompt=system_prompt, user_prompt=user_prompt)\n",
    "    # call_llm_optimizer always returns a list\n",
    "    if not outputs:\n",
    "        return None\n",
    "    return outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11f68c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 7/600 [01:19<1:47:26, 10.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in LLM call for index 6: 'NoneType' object has no attribute 'strip'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 330/600 [1:37:24<36:16,  8.06s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in LLM call for index 329: 'NoneType' object has no attribute 'strip'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 363/600 [1:42:56<38:05,  9.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in LLM call for index 362: 'NoneType' object has no attribute 'strip'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [2:51:09<00:00, 17.12s/it]   \n"
     ]
    }
   ],
   "source": [
    "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    original_text = row[\"text\"]\n",
    "    \n",
    "    # Build the 6 user prompts for this essay\n",
    "    user_prompts = [\n",
    "        build_prompt(prompt_features={\"ESSAY_TEXT\": original_text}, file=PROMPT_DIR + USER_PROMPTS_PREFIX.format(i))\n",
    "        for i in range(1, 7)\n",
    "    ]\n",
    "    \n",
    "    # Call the LLM once per style level (simplest & most modular)\n",
    "    try:\n",
    "        with ThreadPoolExecutor(max_workers=6) as executor:\n",
    "            futures = [\n",
    "                executor.submit(rewrite_one_style, prompt, SYSTEM_PROMPT)\n",
    "                for prompt in user_prompts\n",
    "            ]\n",
    "            # preserve order: futures[0] -> style 1, futures[1] -> style 2, etc.\n",
    "            outputs = [f.result() for f in futures]\n",
    "    except Exception as e:\n",
    "        print(f\"Error in LLM call for index {index}: {e}\")\n",
    "        # Mark error in all 6 dfs for this row and continue\n",
    "        for i in range(1, 7):\n",
    "            dfs_by_score[i].at[index, \"rewritten_text\"] = \"error occurred\"\n",
    "            dfs_by_score[i].at[index, \"content_preserved\"] = None\n",
    "        continue\n",
    "\n",
    "    # --- Evaluate content preservation across the 6 rewrites ---\n",
    "    try:\n",
    "        content_preserved_list = evaluate_rewrites(\n",
    "            original_text=original_text,\n",
    "            rewritten_texts=outputs,\n",
    "            file=EVALUATION_PROMPT_PATH\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error in evaluation for index {index}: {e}\")\n",
    "        # If evaluation fails, set all content_preserved to None and still keep the rewrites\n",
    "        content_preserved_list = [None] * 6\n",
    "\n",
    "    # --- Store results: for each style i, put its rewrite in the corresponding df ---\n",
    "    for i, (rewritten_text, cp) in enumerate(zip(outputs, content_preserved_list), start=1):\n",
    "        dfs_by_score[i].at[index, \"rewritten_text\"] = rewritten_text\n",
    "\n",
    "        if cp is None:\n",
    "            dfs_by_score[i].at[index, \"content_preserved\"] = None\n",
    "        else:\n",
    "            dfs_by_score[i].at[index, \"content_preserved\"] = (cp == \"YES\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7becf417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df_combined to csv\n",
    "for i, df_x in dfs_by_score.items():\n",
    "    df_x.to_csv(f'{SAVE_DIR}raw_{SAVE_NAME.format(i)}', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34833586",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_indices = set()\n",
    "\n",
    "# 1) Collect bad indices across all dfs\n",
    "for i, df in dfs_by_score.items():\n",
    "    col = df[\"rewritten_text\"].astype(str)\n",
    "\n",
    "    mask_error = col.str.contains(\"error occurred\", case=False, na=False)\n",
    "    mask_nan   = df[\"rewritten_text\"].isna()\n",
    "    mask_empty = col.str.strip().eq(\"\")\n",
    "    mask_short = col.str.len() < 20\n",
    "\n",
    "    mask_bad = mask_error | mask_nan | mask_empty | mask_short\n",
    "\n",
    "    bad_indices.update(df.index[mask_bad])\n",
    "\n",
    "# 2) Drop those rows from ALL dfs and save cleaned versions\n",
    "for i, df in dfs_by_score.items():\n",
    "    df_cleaned = df.loc[~df.index.isin(bad_indices)].copy()\n",
    "    df_cleaned.to_csv(f\"{SAVE_DIR}{SAVE_NAME.format(i)}\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
