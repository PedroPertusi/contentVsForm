{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76acd20f",
   "metadata": {},
   "source": [
    "\n",
    "## **Notebook Overview — Fixed-Effects Decomposition and Bootstrap**\n",
    "\n",
    " **Purpose:**\n",
    " This notebook estimates a fixed-effects (FE) model to decompose SES-related differences in essay scores  \n",
    " into *content*, *style*, and *other* components, and optionally computes bootstrap confidence intervals.\n",
    "\n",
    "**How to Use:**\n",
    " - **Run up to Cell 3** if you only need the **decomposition dataset**:  \n",
    "   This will fit the FE model once, append the fixed effects (`fe_essay`, `fe_k`) and residuals (`u`)  \n",
    "   to each observation, and save the full augmented dataset as a CSV file.  \n",
    "   --> Output file: `decomp_rows_with_fe.csv`\n",
    "\n",
    " - **Run all cells (1–7)** if you also want to **compute bootstrap confidence intervals**:  \n",
    "   This will perform 500 clustered bootstrap replications at the essay level (resampling essays  \n",
    "   with all rewrites), re-estimate the FE model each time, and produce percentile-based 95% CIs  \n",
    "   for the total, content, style, and others decomposition metrics.\n",
    "\n",
    " **Outputs:**\n",
    " - `decomp_rows_with_fe.csv` --> Row-level data with FE components for all essays and rewrites.  \n",
    " - `summary_df` --> Compact table with decomposition point estimates and bootstrap 95% CIs (Both in .csv and .tex formats)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f151cd",
   "metadata": {},
   "source": [
    "### **Cell 1 — Imports & Configuration**\n",
    "\n",
    "**Purpose:**\n",
    "- Import all core Python libraries used throughout the notebook.  \n",
    "- Prepare tools for data handling (`pandas`, `numpy`), model estimation (`pyfixest`), and progress visualization (`tqdm`).  \n",
    "- Include utilities for parallel computation (`joblib`) and statistical modeling (`statsmodels`).  \n",
    "- Initialize a global random number generator (`rng`) with a fixed seed to ensure reproducibility of bootstrap samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b9331e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import statsmodels.api as sm\n",
    "from pyfixest.estimation import feols\n",
    "\n",
    "# Reproducibility\n",
    "rng = np.random.default_rng(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76890d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = \"../data/results/\"\n",
    "\n",
    "df_path = \"../data/results/data_no_desc_scored_final.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72434307",
   "metadata": {},
   "source": [
    "### **Cell 2 — Load & Basic Preparation**\n",
    "\n",
    "**Purpose:**\n",
    "- Load the scored dataset containing essay-level and rewrite-level information.  \n",
    "- Sort rows by `essay_id` and `k` so each essay’s rewrites appear sequentially, ensuring consistent ordering for later fixed-effect estimation and bootstrapping.  \n",
    "- Reset the index to maintain a clean 0–N row index after sorting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "671926a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(df_path, encoding=\"ISO-8859-1\", index_col=0)\n",
    "\n",
    "df = df.sort_values([\"essay_id\",\"k\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7905e5",
   "metadata": {},
   "source": [
    "### **Cell 3 — Fit Fixed-Effects Model and Extract Baseline Estimates**\n",
    "\n",
    "**Purpose:**\n",
    "- Estimate a two-way fixed-effects model using `pyfixest` to control for essay-specific and prompt-specific variation.  \n",
    "- Model specification: `score_high_full ~ 1 | essay_id + k`.  \n",
    "- Retrieve the estimated fixed effects from the model output (`fes`):  \n",
    "  - `fe_essay` → captures essay-level (content) differences.  \n",
    "  - `fe_k` → captures prompt-level (context) effects.  \n",
    "- Map these fixed effects back to each observation in the DataFrame for later decomposition.  \n",
    "- Compute residuals `u = score_high_full – fe_essay`, representing the style component once essay-level effects are removed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14117e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "functools.partial(<function summary at 0x1555dc720>, models=[<pyfixest.estimation.feols_.Feols object at 0x158f01fd0>])\n",
      "Saved augmented dataset - ../data/results/decomp_rows_with_fe.csv\n"
     ]
    }
   ],
   "source": [
    "# FE regression\n",
    "res = feols(\"score_high_full ~ 1 | essay_id + k\", data=df)\n",
    "print(res.summary)\n",
    "\n",
    "fes = res.fixef()\n",
    "fe_essay_map = fes[\"C(essay_id)\"]  #\n",
    "fe_k_map = fes[\"C(k)\"]\n",
    "\n",
    "# Map back to rows\n",
    "df[\"fe_essay\"] = df[\"essay_id\"].map(fe_essay_map)\n",
    "df[\"fe_k\"] = df[\"k\"].map(fe_k_map)\n",
    "\n",
    "# Style residual (after removing essay FE only, as in your code)\n",
    "df[\"u\"] = df[\"score_high_full\"] - df[\"fe_essay\"]\n",
    "\n",
    "# Compute diff column\n",
    "df[\"diff\"] = df[\"score_high_full\"] - df[\"score_low_full\"]\n",
    "\n",
    "# Save DataFrame with FEs and residuals\n",
    "df.to_csv(out_path + \"decomp_rows_with_fe.csv\", index=False)\n",
    "print(f\"Saved augmented dataset - {out_path}decomp_rows_with_fe.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c25f2cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a small utility function to compute conditional means while safely handling missing values (`NaN`). \n",
    "\n",
    "def mean_mask(s: pd.Series, mask: pd.Series) -> float:\n",
    "    s2 = s[mask & s.notna()]\n",
    "    return float(s2.mean()) if len(s2) else np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870ed093",
   "metadata": {},
   "source": [
    "\n",
    "### **Cell 4 — Decomposition**\n",
    "\n",
    "**Purpose:**\n",
    "- Compute the SES gap and its decomposition into **content**, **style**, and **others** using the FE outputs.  \n",
    "- Restrict the decomposition to **k == 0** for comparability.  \n",
    "- Return both **absolute gaps** and **shares** (each component divided by the total gap).\n",
    "\n",
    "**Steps:**\n",
    " - **A) Total gap (k==0):**  \n",
    "   - Group 0 = high-SES `score_high_full` where `low_SES==0`.  \n",
    "   - Group 1 = low-SES  `score_low_full`  where `low_SES==1`.  \n",
    "   - `total_gap = mean(group 0) – mean(group 1)`.\n",
    " - **B) Content gap (k==0):**  \n",
    "   - Compare `fe_essay` means between SES groups; essay FE proxies content.  \n",
    "   - `content_gap = mean(fe_essay | high-SES) – mean(fe_essay | low-SES)`.\n",
    " - **C) Style gap (k==0):**  \n",
    "   - Use residual `u = score_high_full – fe_essay` as a style proxy.  \n",
    "   - `style_gap = mean(u | high-SES) – mean(u | low-SES)`.\n",
    " - **D) Others gap (k==0, within high-SES):**  \n",
    "   - Difference between `score_high_full` and `score_low_full` (same SES group).  \n",
    "   - `others_gap = mean(score_high_full) – mean(score_low_full)` for `low_SES==0`.\n",
    "\n",
    "**Outputs:**\n",
    " - Dictionary with absolute gaps: `total_gap`, `content_gap`, `style_gap`, `others_gap`.  \n",
    " - Shares: `share_content`, `share_style`, `share_others` (component / total_gap).  \n",
    " - `point_est` stores the baseline (non-bootstrap) decomposition for the full sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c1cf638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_gap': 0.6572251073631734,\n",
       " 'content_gap': 0.518098210274891,\n",
       " 'style_gap': 0.10459843024325868,\n",
       " 'others_gap': 0.06186198976259494,\n",
       " 'share_content': 0.7883116522336344,\n",
       " 'share_style': 0.15915160433068948,\n",
       " 'share_others': 0.0941260293764323}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_decomposition_point_estimates(df_in: pd.DataFrame):\n",
    "    df_ = df_in.copy()\n",
    "\n",
    "    # --- A) total gap (k==0)\n",
    "    mA = (df_[\"low_SES\"]==0) & (df_[\"k\"]==0) & df_[\"score_high_full\"].notna()\n",
    "    mB = (df_[\"low_SES\"]==1) & (df_[\"k\"]==0) & df_[\"score_low_full\"].notna()\n",
    "    tmp_vals = pd.Series(np.nan, index=df_.index)\n",
    "    tmp_vals[mA] = df_.loc[mA, \"score_high_full\"].astype(float)\n",
    "    tmp_vals[mB] = df_.loc[mB, \"score_low_full\"].astype(float)\n",
    "\n",
    "    group = pd.Series(np.nan, index=df_.index)\n",
    "    group[mA] = 0\n",
    "    group[mB] = 1\n",
    "\n",
    "    m0 = (group==0)\n",
    "    m1 = (group==1)\n",
    "\n",
    "    mean0 = mean_mask(tmp_vals, m0)\n",
    "    mean1 = mean_mask(tmp_vals, m1)\n",
    "    total_gap = mean0 - mean1\n",
    "\n",
    "    # --- B) content gap (k==0), use essay FE\n",
    "    mK0 = (df_[\"k\"]==0)\n",
    "    content_mean0 = mean_mask(df_[\"fe_essay\"], mK0 & (df_[\"low_SES\"]==0))\n",
    "    content_mean1 = mean_mask(df_[\"fe_essay\"], mK0 & (df_[\"low_SES\"]==1))\n",
    "    content_gap = content_mean0 - content_mean1\n",
    "\n",
    "    # --- C) style gap via residual u = score_high_full - fe_essay\n",
    "    u_mean0 = mean_mask(df_[\"u\"], mK0 & (df_[\"low_SES\"]==0))\n",
    "    u_mean1 = mean_mask(df_[\"u\"], mK0 & (df_[\"low_SES\"]==1))\n",
    "    style_gap = u_mean0 - u_mean1\n",
    "\n",
    "    # --- D) others gap (your original \"diff\" within high-SES at k==0)\n",
    "    m_last = (df_[\"low_SES\"]==0) & (df_[\"k\"]==0)\n",
    "    sh_mean = mean_mask(df_[\"score_high_full\"], m_last)\n",
    "    sl_mean = mean_mask(df_[\"score_low_full\"],  m_last)\n",
    "    others_gap = sh_mean - sl_mean\n",
    "\n",
    "    return {\n",
    "        \"total_gap\": total_gap,\n",
    "        \"content_gap\": content_gap,\n",
    "        \"style_gap\": style_gap,\n",
    "        \"others_gap\": others_gap,\n",
    "        \"share_content\": content_gap/total_gap if pd.notna(total_gap) and total_gap!=0 else np.nan,\n",
    "        \"share_style\": style_gap/total_gap   if pd.notna(total_gap) and total_gap!=0 else np.nan,\n",
    "        \"share_others\": others_gap/total_gap  if pd.notna(total_gap) and total_gap!=0 else np.nan,\n",
    "    }\n",
    "\n",
    "point_est = compute_decomposition_point_estimates(df)\n",
    "point_est\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8ed98a",
   "metadata": {},
   "source": [
    "\n",
    "### **Cell 5 — Bootstrap (Clustered at essay_id; Parallelized, 500 Reps, 2.5–97.5% CI)**\n",
    "\n",
    "**Purpose:**\n",
    "- Estimate uncertainty for the decomposition metrics using **clustered bootstrapping** at the essay level.  \n",
    "- Each bootstrap sample resamples essays (clusters) **with replacement**, keeping all their rewrites together.  \n",
    "- The process re-fits the fixed-effects model on each resample, computes the decomposition again,  \n",
    "  and aggregates results across 500 iterations to form **95% confidence intervals**.\n",
    "\n",
    "**Steps:**\n",
    "1. Reset DataFrame index and precompute a dictionary mapping each `essay_id` to its row indices  \n",
    "2. Define the helper function `one_rep(seed)` that performs a **single bootstrap iteration**:  \n",
    "   - Samples `essay_id`s with replacement (cluster bootstrap).  \n",
    "   - Extracts the corresponding rows efficiently via `.iloc`.  \n",
    "   - Re-fits the FE model (`score_high_full ~ 1 | essay_id + k`).  \n",
    "   - Computes essay and prompt fixed effects, residuals, and decomposition metrics.  \n",
    "3. Generate a list of independent random seeds (one per iteration) to ensure reproducibility  \n",
    "   across parallel workers.  \n",
    "4. Use **Joblib’s `Parallel`** and **`delayed`** utilities to run all 500 bootstrap replications concurrently  \n",
    "   (`n_jobs=-1` automatically uses all available CPU cores).  \n",
    "5. Convert the resulting list of dictionaries into a single DataFrame (`boot_df_stats`),  \n",
    "   where each row corresponds to one bootstrap iteration and each column to a decomposition metric.  \n",
    "6. Compute percentile-based confidence intervals (2.5% and 97.5%) for every statistic,  \n",
    "   storing them in `ci_bounds` as a compact dictionary of lower–upper tuples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2718461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 500 bootstrap iterations in parallel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:47<00:00,  4.66it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total_gap': (np.float64(0.6320802301989173), np.float64(0.6837142444898432)),\n",
       " 'content_gap': (np.float64(0.49303931617708774),\n",
       "  np.float64(0.5451547111003249)),\n",
       " 'style_gap': (np.float64(0.09617602620540146),\n",
       "  np.float64(0.11219516620095764)),\n",
       " 'others_gap': (np.float64(0.05783373141660024),\n",
       "  np.float64(0.0662591759843586)),\n",
       " 'share_content': (np.float64(0.7742281514369145),\n",
       "  np.float64(0.8026585149995842)),\n",
       " 'share_style': (np.float64(0.14666320021384152),\n",
       "  np.float64(0.17290491273081907)),\n",
       " 'share_others': (np.float64(0.0874803281322524),\n",
       "  np.float64(0.100877995749596))}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.reset_index(drop=True)\n",
    "\n",
    "essay_to_idx = df.groupby(\"essay_id\").indices\n",
    "essay_ids = np.array(list(essay_to_idx.keys()))\n",
    "n_clusters = len(essay_ids)\n",
    "\n",
    "\n",
    "def one_rep(seed: int) -> dict:\n",
    "    \"\"\"\n",
    "    Performs ONE bootstrap iteration:\n",
    "    1. Samples essay_ids with replacement.\n",
    "    2. Collects all their rows from df.\n",
    "    3. Fits the fixed-effects model.\n",
    "    4. Computes the decomposition metrics.\n",
    "    5. Returns a dictionary with the results.\n",
    "    \"\"\"\n",
    "\n",
    "    rng_local = np.random.default_rng(seed)\n",
    "\n",
    "    sampled = rng_local.choice(essay_ids, size=n_clusters, replace=True)\n",
    "\n",
    "    idx = np.concatenate([essay_to_idx[eid] for eid in sampled])\n",
    "\n",
    "    df_b = df.iloc[idx]\n",
    "\n",
    "    res_b = feols(\"score_high_full ~ 1 | essay_id + k\", data=df_b)\n",
    "\n",
    "    fes_b = res_b.fixef()\n",
    "    fe_essay_b = fes_b[\"C(essay_id)\"]  # essay-level effects\n",
    "    fe_k_b = fes_b[\"C(k)\"] # prompt-level effects\n",
    "\n",
    "    df_b = df_b.copy()\n",
    "\n",
    "    df_b[\"fe_essay\"] = df_b[\"essay_id\"].map(fe_essay_b)\n",
    "    df_b[\"fe_k\"] = df_b[\"k\"].map(fe_k_b)\n",
    "\n",
    "    df_b[\"u\"] = df_b[\"score_high_full\"] - df_b[\"fe_essay\"]\n",
    "\n",
    "    # Compute total/content/style/others gaps and shares on this sample\n",
    "    # Returns a dict like {'total_gap': x, 'content_gap': y, 'style_gap': z, ...}\n",
    "    return compute_decomposition_point_estimates(df_b)\n",
    "\n",
    "\n",
    "# --- Parallel execution setup ---\n",
    "B = 500  # number of bootstrap iterations\n",
    "\n",
    "# Create an array of random seeds (one per bootstrap replication)\n",
    "# This ensures each parallel worker uses a unique, reproducible seed\n",
    "seeds = rng.integers(0, 2**32 - 1, size=B, dtype=np.uint64)\n",
    "\n",
    "print(f\"Running {B} bootstrap iterations in parallel...\")\n",
    "\n",
    "# Run the bootstrap in parallel using all available CPU cores\n",
    "# - n_jobs=-1  = use all cores\n",
    "# - backend='loky' = safe multiprocessing backend for heavy tasks\n",
    "# - delayed(one_rep)(int(s)) = schedules one_rep(seed) for each seed\n",
    "# - tqdm(seeds)  = adds progress bar\n",
    "results = Parallel(n_jobs=-1, backend=\"loky\")(\n",
    "    delayed(one_rep)(int(s)) for s in tqdm(seeds)\n",
    ")\n",
    "\n",
    "# After all parallel jobs finish, 'results' is a list of dicts\n",
    "boot_df_stats = pd.DataFrame(results)\n",
    "\n",
    "# --- Compute bootstrap confidence intervals ---\n",
    "ci_bounds = {}\n",
    "for col in boot_df_stats.columns:\n",
    "    low, high = np.nanpercentile(boot_df_stats[col], [2.5, 97.5])\n",
    "    ci_bounds[col] = (low, high)\n",
    "\n",
    "ci_bounds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f60a0b8",
   "metadata": {},
   "source": [
    "### **Cell 7 — Summary Table (Point Estimates + Bootstrap Confidence Intervals)**\n",
    "\n",
    "**Purpose:**\n",
    "- Combine the **baseline decomposition point estimates** (from Cell 5)  \n",
    "with the **bootstrap confidence intervals** (from Cell 6) into one concise summary table.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25236114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stat</th>\n",
       "      <th>point_estimate</th>\n",
       "      <th>ci_2.5</th>\n",
       "      <th>ci_97.5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>total_gap</td>\n",
       "      <td>0.657</td>\n",
       "      <td>0.632</td>\n",
       "      <td>0.684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>content_gap</td>\n",
       "      <td>0.518</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>style_gap</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>others_gap</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>share_content</td>\n",
       "      <td>0.788</td>\n",
       "      <td>0.774</td>\n",
       "      <td>0.803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>share_style</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>share_others</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            stat  point_estimate  ci_2.5  ci_97.5\n",
       "0      total_gap           0.657   0.632    0.684\n",
       "1    content_gap           0.518   0.493    0.545\n",
       "2      style_gap           0.105   0.096    0.112\n",
       "3     others_gap           0.062   0.058    0.066\n",
       "4  share_content           0.788   0.774    0.803\n",
       "5    share_style           0.159   0.147    0.173\n",
       "6   share_others           0.094   0.087    0.101"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_rows = []\n",
    "for name, pe in point_est.items():\n",
    "    lo, hi = ci_bounds[name]\n",
    "    summary_rows.append({\n",
    "        \"stat\": name,\n",
    "        \"point_estimate\": round(pe, 3),\n",
    "        \"ci_2.5\": round(lo, 3),\n",
    "        \"ci_97.5\": round(hi, 3)\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a8dc350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save summary_df to csv\n",
    "summary_df.to_csv(out_path + \"decomposition_bootstrap_summary.csv\", index=False)\n",
    "\n",
    "# save summary_df to latex\n",
    "summary_df.to_latex(out_path + \"decomposition_bootstrap_summary.tex\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
